---
title: "Interval MDS"
author: 
- Jan de Leeuw - University of California Los Angeles
date: '`r paste("Started November 24 2023, Version of",format(Sys.Date(),"%B %d, %Y"))`'
output:
  bookdown::pdf_document2:
    latex_engine: lualatex 
    includes:
      in_header: preamble.tex
    keep_tex: yes
    number_sections: yes
  bookdown::html_document2:
    keep_md: yes
    css: preamble.css
    number_sections: yes
graphics: yes
mainfont: Times New Roman
fontsize: 12pt
bibliography: ["mypubs.bib","total.bib"]
editor_options: 
  markdown: 
    wrap: 72
---

# Problem

In the transformation phase of interval MDS we want to minimize
\begin{equation}
\sigma(\alpha,\beta):=\sum_{k=1}^Kw_k(\alpha\delta_k+\beta-d_k)^2
(\#eq:loss)
\end{equation}
over the inequality constraints $\alpha\geq 0$ and
$\alpha\delta_k+\beta\geq 0$ and the normalization constraint
\begin{equation}
\sum_{k=1}^Kw_k(\alpha\delta_k+\beta)^2=1.
\end{equation}
The $w_k$ in definition \@ref(eq:loss) are non-negative weights that
add up to one. Note that some or all of the $\delta_k$ can be negative.

The theory of normalized cone regression (@deleeuw_U_75a,
@bauschke_bui_wang_18, @deleeuw_E_19d) shows that we can ignore the
normalization constraint and impose only the inequality constraints. We
find the solution to the normalized problem by normalizing the solution
of the unnormalized problem.

The inequality constraints are obviously equivalent to $\alpha\geq 0$
and $\alpha\delta_{\text{min}}+\beta\geq 0$, with $\delta_{\text{min}}$
the smallest of the $\delta_k$. Now a little change of variables.
Define
$\gamma:=\alpha\delta_{\text{min}}+\beta$, and let
$e_k:=\delta_k-\delta_{\text{min}}$. Note that
$e_k\geq 0$ for all $k$.

The cone of vectors $\alpha\delta_k+\beta$ with $\alpha\geq 0$ and
$\alpha\delta_k+\beta\geq 0$ is the same as the cone $\alpha e_k+\gamma$
with $\alpha\geq 0$ and $\gamma\geq 0$. Thus our original (unnormalized)
problem is equivalent to minimization of
\begin{equation}
\sigma(\alpha,\gamma)=\sum_{k=1}^Kw_k(\alpha e_k+\gamma-d_k)^2
\end{equation}
over $\alpha\geq 0$ and $\gamma\geq 0$, i.e. over the non-negative 
orthant. Of course the normalization condition becomes
\begin{equation}
\sum_{k=1}^Kw_k(\alpha e_k+\gamma)^2=1.
\end{equation}

# Equations

So let's first consider the unnormalized problem of minimizing
$\sigma$ over $\alpha\geq 0$ and $\gamma\geq 0$.

We start with the trivial observation that the minimum of any function
over the non-negative orthant in two-dimensional space is attained
either in the interior, i.e. the positive orthant, or on one of the two
rays emanating from the origin along the axes, or at the intersection of
these rays, i.e. at the origin.

Now, in a more compact notation, 
\begin{equation}
\sigma(\alpha,\gamma)=\alpha^2[e^2]+\gamma^2+[d^2]+2\alpha\gamma[e]-2\alpha[ed]-2\gamma[d],
\end{equation} 
where the square brackets indicate weighted means. We also assume that the MDS problem is 
non-trivial, by which we mean in this context that both $[e]$ and $[d]$ are positive. If $[e]$ is zero all $\delta_k$ are equal, if $[d]$ is zero all $d_k$ are zero. Just in case, if all $e_k$ are zero
then the optimum $\gamma$ is $\gamma_0=[d]$, while $\alpha_0$ is arbitrary. The minimum is equal to
$\sigma_0=[d^2]-[d]^2$.

If the minimum is attained in the interior then the gradient
must vanish at the minimum. The gradient vanishes at
\begin{align}
\alpha_0&=\frac{[ed]-[e][d]}{[e^2]-[e]^2},\\
\gamma_0&=[d]-\alpha_0[e].
\end{align} 
If $\alpha_0\geq 0$ and $\gamma_0\geq 0$ we are done and
we have found the required minimum. The minimum
value of $\sigma$ is
\begin{equation}
\sigma_0=[d^2]-[d]^2-\frac{([ed]-[e][d])^2}{[e^2]-[e]^2}.
\end{equation}

If $(\alpha_0,\gamma_0)$ is not in the non-negative orthant, the minimum
either occurs on the line $\alpha=0$ or on the line $\gamma=0$, or at
their intersection.

The minimum on the line $\alpha=0$ occurs at $\alpha_1=0$ and $\gamma_1=[d]$, and is equal
to 
\begin{equation}
\sigma_1=\sigma(\alpha_1,\gamma_1)=[d^2]-[d]^2.
\end{equation} 
Since $[d]$ is positive the point $(\alpha_1,\gamma_1)$
is in the non-negative orthant, and thus always feasible. From the normalization
condition we find that the solution of the normalized problem on
$\alpha=0$ is the point $(0,1)$.


The minimum on the line $\gamma=0$ occurs at $\gamma_2=0$ and 
\begin{equation}
\alpha_2=\frac{[ed]}{[e^2]},
\end{equation} 
and is equal to \begin{equation}
\sigma_2:=\sigma(\alpha_2,0)=[d^2]-\frac{[ed]^2}{[e^2]}.
\end{equation}
Again $\alpha_2\geq 0$ and thus $(\alpha_2,\gamma_2)$ is always feasible.
the solution to the normalized problem on $\gamma=0$ is the point
$([e^2]^{-\frac12}, 0)$.

Thus, summarizing, if $(\alpha_0,\gamma_0)$ is not feasible then the
minimum of the unnormalized problem is at $(\alpha_1,\beta_1)$ 
if $\sigma_1<\sigma_2$ and at
$(\alpha_2,\beta_2)$ if $\sigma_2<\sigma_1$. There is C code (for the
unweighted case of the smacof project) in the appendix.

# Appendix: Code

```{c file_auxilary, code = readLines("interval.c"), eval = FALSE}
```


# References
